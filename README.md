# ğŸ”¢ Handwritten Digit Classification with ML

This project focuses on **classifying handwritten digits** (0â€“9) using different Machine Learning algorithms.  
It is based on the classic **Digits Dataset** provided by `scikit-learn`.  

The goal is to **train, evaluate, and tune models** such as:  
ğŸŒ² Random Forest, ğŸŒ³ Decision Tree, and ğŸ± CatBoost â€” then compare their performance using accuracy, F1-scores, and confusion matrices.  

---

## âœ¨ What this Project Does
- ğŸ“¥ Loads the **digits dataset** with `sklearn.datasets.load_digits`  
- ğŸ–¼ï¸ Displays a sample digit image with its label  
- âš–ï¸ Scales the features using **MinMaxScaler**  
- ğŸ§  Trains and evaluates 3 ML models:  
  - ğŸŒ² **Random Forest Classifier**  
  - ğŸŒ³ **Decision Tree Classifier**  
  - ğŸ± **CatBoost Classifier**  
- ğŸ“Š Prints out:
  - Accuracy  
  - Macro F1-score  
  - Detailed classification report  
- ğŸ” Performs **GridSearchCV** for hyperparameter tuning  
- ğŸ¨ Plots confusion matrices for tuned models  

---

## ğŸ“¦ Requirements

Before running, install the following Python libraries:  

pandas
numpy
matplotlib
scikit-learn
catboost

python
Copier le code

ğŸ‘‰ Install all at once with:
```bash
pip install pandas numpy matplotlib scikit-learn catboost
â–¶ï¸ How to Run
Save the script as main.py and run:

bash
Copier le code
python main.py
When executed, the program will:

ğŸ§¾ Print dataset shape (X, y) and first labels

ğŸ‘ï¸ Show an example digit image

ğŸ¤– Train baseline models (RandomForest, DecisionTree, CatBoost)

ğŸ“ˆ Print metrics (Accuracy + F1-score + Classification Report)

ğŸ”§ Use GridSearchCV to find best hyperparameters

ğŸ† Train tuned models and compare performance

ğŸ–¼ï¸ Plot confusion matrices for each tuned model

ğŸ“Š Example Output
âœ… Dataset Info
yaml
Copier le code
Shape of X: (1797, 64)
Shape of y: (1797,)
First 10 labels: [0 1 2 3 4 5 6 7 8 9]
ğŸ¤– Model Accuracy (example)
yaml
Copier le code
RandomForest Accuracy: 0.97
DecisionTree Accuracy: 0.84
CatBoost Accuracy: 0.98
ğŸ”§ Tuned Parameters (example)
rust
Copier le code
Best Params for RandomForest: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 2}
Best Params for DecisionTree: {'max_depth': 20, 'min_samples_split': 2}
Best Params for CatBoost: {'iterations': 600, 'learning_rate': 0.1, 'depth': 6}
ğŸ“‰ Confusion Matrix
The confusion matrix will be displayed as a heatmap showing how well each digit is classified.

ğŸ“– Explanation of Models
ğŸŒ² Random Forest: An ensemble method using multiple decision trees to improve accuracy and reduce overfitting.

ğŸŒ³ Decision Tree: A simple model that splits data based on feature thresholds, good for interpretability but prone to overfitting.

ğŸ± CatBoost: A gradient boosting algorithm that handles categorical data well and often outperforms traditional models.

ğŸ”® Future Improvements
ğŸ§  Add Deep Learning models (CNNs with TensorFlow/PyTorch) for higher accuracy

ğŸ’¾ Save trained models with joblib for reuse

ğŸŒ Create a small web app (Streamlit/Flask) to predict digits interactively

ğŸ“± Extend to mobile apps for handwriting recognition


